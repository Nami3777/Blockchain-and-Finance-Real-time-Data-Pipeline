# ğŸ“Š Real-Time Stock Tracking Pipeline  

This project demonstrates an **end-to-end data engineering pipeline** for real-time stock tracking, especially blockchain and finanace related cooperations ( Coinbase Global Inc., Marathon Digital Holdings Inc., MicroStrategy Inc. and BlackRock Inc.).  
The pipeline leverages modern data tools to simulate how **streaming data can be ingested, orchestrated, transformed, and stored** for analytics and downstream use cases.  

---

## ğŸš€ Architecture  

![Architecture Diagram](./assets/architecture_diagram.png)
Finhub API (Data Source) â†’ Kafka â†’ MinIO (Bronze) â†’ Airflow DAG â†’ Snowflake (Bronze Table) â†’ dbt (Silver/Gold Models)
All core components (Kafka, MinIO, Airflow, and dbt) were deployed and orchestrated using **Docker containerization**, ensuring reproducibility, isolated environments, and easy scalability.

- **Kafka** â†’ Streams ticker data from external APIs  
- **MinIO** â†’ S3-compatible object store for raw JSON files  
- **Airflow** â†’ Orchestrates ingestion & loading into Snowflake  
- **Snowflake** â†’ Cloud data warehouse for staging and querying  
- **dbt** â†’ Transformation into structured models (Silver/Gold layers)  

---

## ğŸ› ï¸ Tech Stack  

- **Data Streaming**: Apache Kafka  
- **Storage**: MinIO (S3 API)  
- **Orchestration**: Apache Airflow  
- **Data Warehouse**: Snowflake  
- **Transformation**: dbt  

---

## ğŸ“‚ Project Structure  

\\\
â”œâ”€â”€ dags/ # Airflow DAGs for ingestion
â”œâ”€â”€ dbt/ # dbt models for transformations
â”œâ”€â”€ infra/ # Infrastructure config (Docker, MinIO, Airflow)
â”œâ”€â”€ venv/ # Local Python virtual environment (for testing/debugging)
â””â”€â”€ requirements.txt # Requirements for project
\\\
---

## âš™ï¸ Workflow  

1. **Kafka** streams stock ticker data in near real-time.  
2. **MinIO** stores raw JSON snapshots in a Bronze bucket.  
3. **Airflow DAG** downloads files from MinIO and executes Snowflake `PUT` + `COPY INTO`.  
4. **Snowflake** holds the Bronze raw table.  
5. **dbt models** parse JSON into structured Silver/Gold tables for business-ready analysis.  

---

## ğŸ“Œ Key Features  

- Automated ingestion pipeline with **Airflow scheduling**  
- Cloud-native storage & compute using **MinIO + Snowflake**  
- SQL-first transformations with **dbt**  
- Modular **Bronzeâ€“Silverâ€“Gold data model architecture**  

---

## âš ï¸ Known Limitations  

- Short observation window â†’ limited volatility, many flat charts  
- Snowflake `COPY_HISTORY` queries sometimes return partial logs  
- Data refresh not fully automated beyond Airflow  

---

## ğŸ”® Future Enhancements  
 
- **Direct Kafka â†’ Snowflake connector** (removes MinIO dependency)  
- **Longer historical collection** for richer trend and volatility analysis  
- **Monitoring/alerting** for Airflow DAG failures  
- **Extend sources**: crypto APIs, traditional finance APIs  

---

## ğŸ¯ This project showcases:  

- **Data Engineering skills**: streaming ingestion, orchestration, cloud warehouses, transformations  
- **Consulting mindset**: problem framing, modular design, scalability in mind  
- **Real-world alignment**: pipeline pattern matches how modern data teams handle real-time financial data  

---


